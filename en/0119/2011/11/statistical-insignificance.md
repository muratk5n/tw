# Statistical Insignificance

It’s science’s dirtiest secret: The “scientific method” of testing
hypotheses by statistical analysis stands on a flimsy
foundation. Statistical tests are supposed to guide scientists in
judging whether an experimental result reflects some real effect or is
merely a random fluke, but the standard methods mix mutually
inconsistent philosophies and offer no meaningful basis for making
such decisions. Even when performed correctly, statistical tests are
widely misunderstood and frequently misinterpreted. As a result,
countless conclusions in the scientific literature are erroneous, and
tests of medical dangers or treatments are often contradictory and
confusing [..].

“There is increasing concern,” declared epidemiologist John Ioannidis
[.] “that in modern research, false findings may be the majority or
even the vast majority of published research claims.” Ioannidis
claimed to prove that more than half of published findings are false
[..]

Statistical significance is a phrase that every science graduate
student learns, but few comprehend. While its origins stretch back at
least to the 19th century, the modern notion was pioneered by the
mathematician Ronald A. Fisher in the 1920s. His original interest was
agriculture. He sought a test of whether variation in crop yields was
due to some specific intervention (say, fertilizer) or merely
reflected random factors beyond experimental control.

Fisher first assumed that fertilizer caused no difference — the “no
effect” or “null” hypothesis. He then calculated a number called the P
value, the probability that an observed yield in a fertilized field
would occur if fertilizer had no real effect. If P is less than .05 —
meaning the chance of a fluke is less than 5 percent — the result
should be declared “statistically significant,” Fisher arbitrarily
declared, and the no effect hypothesis should be rejected, supposedly
confirming that fertilizer works.

Fisher’s P value eventually became the ultimate arbiter of credibility
for science results of all sorts [..] “That test itself is neither
necessary nor sufficient for proving a scientific result,” asserts
Stephen Ziliak, an economic historian at Roosevelt University in
Chicago.
